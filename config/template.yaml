#### This is a minimal template explaining each of the training args ####

# True if you want to track experiments using wandb. Else false
wandb: false
# Will only feed the first "max_seq_len" tokens into the model, with the remainder of the document being truncated
max_seq_length: 256
# Certain tasks will have different pipelines- I configure this using the "task" arg
task: snli
# Data will be read from the "data"/"task" folder
data_dir: data
# Directory to save the trained model into. NOTE if this directory exists and contains files within it, the training loop will throw an error to prevent these files from being overwritten
# Set output_dir and run_name to the same value
output_dir: runs/snli2
run_name: runs/snli2
# The model to run. This should be one of the following:
# - A hugginface model name (e.g. bert-base-uncased). These can be found at https://huggingface.co/models
# - A path to a saved model (i.e. runs/snli/checkpoint-2504)
model_name_or_dir: runs/snli1/checkpoint-2504
# Important training arguments
learning_rate: 5.0e-05
per_device_eval_batch_size: 32
per_device_train_batch_size: 32
# These configure whether to run training "do_train", evaluation "do_eval" and run on the test set "do_predict"
do_eval: true
do_predict: true
do_train: true
# More training args
num_train_epochs: 3.0
evaluation_strategy: yes
eval_steps: 500
load_best_model_at_end: true
save_steps: 1000
max_lines: 100000
logging_first_step: true
logging_steps: 10
